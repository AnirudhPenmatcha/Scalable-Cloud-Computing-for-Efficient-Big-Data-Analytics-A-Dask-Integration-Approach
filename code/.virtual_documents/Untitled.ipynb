import optuna
import xgboost as xgb
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from dask.distributed import wait
import time


%%time 
xgb_hpo()


import time
import psutil
import warnings
warnings.filterwarnings("ignore")
%time xgb_hpo_dask() 


def xgb_hpo():
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=50)
    print(study.best_value)


import dask
from dask.distributed import Scheduler, Client, LocalCluster
from dask.distributed import Client, LocalCluster
import dask.dataframe as dd
cluster = LocalCluster(n_workers=2,threads_per_worker=2)
client = Client(cluster)


client.close()
cluster.close()


client


backend_storage = optuna.storages.InMemoryStorage()
dask_storage = optuna.integration.DaskStorage(storage=backend_storage)
def xgb_hpo_dask():
    study = optuna.create_study(direction='maximize',storage=dask_storage,sampler=optuna.samplers.RandomSampler())
    futures = [client.submit(study.optimize, objective, n_trials=1, pure=False) for _ in range(50)]
    _ = wait(futures)
    print(study.best_value)


import xgboost as xgb
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import KFold, cross_val_score
def objective(trial):
    X, y = load_breast_cancer(return_X_y=True)
    params = {
        "n_estimators": 10,
        "verbosity": 0,
        # L2 regularization weight.
        "lambda": trial.suggest_float("lambda", 1e-8, 100.0, log=True),
        # L1 regularization weight.
        "alpha": trial.suggest_float("alpha", 1e-8, 100.0, log=True),
        # sampling according to each tree.
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.2, 1.0),
        "max_depth": trial.suggest_int("max_depth", 2, 10, step=1),
        # minimum child weight, larger the term more conservative the tree.
        "min_child_weight": trial.suggest_float(
            "min_child_weight", 1e-8, 100, log=True
        ),
        "learning_rate": trial.suggest_float("learning_rate", 1e-8, 1.0, log=True),
        # defines how selective algorithm is.
        "gamma": trial.suggest_float("gamma", 1e-8, 1.0, log=True),
        "grow_policy": "depthwise",
        "eval_metric": "logloss",
    }
    clf = xgb.XGBClassifier(**params)
    fold = KFold(n_splits=5, shuffle=True, random_state=0)
    score = cross_val_score(clf, X, y, cv=fold, scoring="accuracy")
    return score.mean()


%time study = xgb_hpo_dask()


import pandas as pd
%%
time data = pd.read_csv("test.csv")
data.head


import dask.dataframe as dd
%time 
df = dd.read_csv('test.csv',sep = "/t",engine = "python")
df.columns
for i in df.columns:
    value_counts = df[i].nunique()
    print(value_counts.compute())



